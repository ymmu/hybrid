---
layout:            post
title:             "17/11/23- TIL - Logistic regression-Titanic 생존자 예측"
enutitle:          "17/11/23- TIL - Logistic regression-Titanic 생존자 예측"
category:          Machine learning
author:            myohyun
tags:              Machine learning
---
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>


## 정리
kaggle에 data science beginner를 위한 분석주제로 Titanic 생존자의 확률을 머신러닝으로 측정해보는 것이었다. 단순히 로지스틱회귀를 배웠기 때문에 한 번 활용해 보고 싶어서 시도해보았다. 코드는 새로 짜지 않고 machine learning in action 책의 코드를 수정해서 돌렸다.
데이터 예측을 한 후 다른 사람들의 답을 보니 같은 dataset으로도 다양한 분석을 보여줘서 흥미로웠다. 또한 내가 결과를 얻으며 간과한 여러 요소들에 대해 정리할 시간을 가졌다.

**데이터를 얻으며 생긴 궁금증들: **

1. 로지스틱 회귀에서 몇몇 개의 데이터가 크면 어떻게 해야할까? 한 두개의 데이터는 뺀다 할지라도  특정한 column 전체가 값이 엄청 클 때 어떤 문제가 생길까? 
2. 누락된 데이터를 어떤식로 채워줄 수 있을까?
3. 데이터에서 column을 선택하는 기준은 어떻게 잡아야 할까? 

**간과하였다 고친 요소들:**

1. 누락된 데이터를 잊고 있다 에러로 잡음

**다른 사람들이 다룬 데이터 분석들:**

1. 다양한 visualization
2. 각 column들 간의 관계 분석 (family size/나이와 생존확률, 각 column끼리의 관계도 등)관계 분석에서 다양한 시각화 구현 방법을 배울 수 있을 것 같다.

**추가할 것들:**

1. 예측값이 나온 후 머신러닝방법을 쓰기 전에 training data의 column으로 분석한 내용과 얼마나 일치하는지 확인하는 작업을 해도 괜찮을 듯 하다.
2. 위의 궁금증들에 대한 답
3. 다른 머신러닝 기법으로 예측한 후 결과가 얼마나 다른지 비교해봐도좋을 듯 하다

# 데이터 불러오기

```python
import csv
import logRegres #
from numpy import *
f=open('train_adj.csv','r', encoding='utf-8')
rdr = csv.reader(f)
```

# dataSet 다듬기
missing data를 채워 넣은 값에 대한 이유가 필요하다.
다른 article에서 다른 유사한 데이터를 참고하여 같은 값을 넣는 경우도 보았다.

```python
#1. training set을 먼저 수정한다. 로지스틱 회귀를 이용할 것이므로 비수치형 column은 삭제한다. 남은 column 수 = 8 
dataMat = []; labelMat = []
fr = open('train_adj.csv','r', encoding='utf-8')

#csv 파일의 행을 하나씩 읽어서 7th column까지는 dataMat에 넣고, 8th column인 survival값은 labelMat에 넣어준다.
lines = fr.readlines()
for n in range(1,len(lines)):
    lineArr = lines[n].strip().split(',')
    dataInLine = []
    for i in range(0,len(lineArr)):
        if i== (len(lineArr)-1):
            labelMat.append(int(lineArr[i]))
        else:
            dataInLine.append(float(lineArr[i]))
    dataMat.append(dataInLine)
dataMatrix = array(dataMat)
```

# Weight 계산
```python
m,n = shape(dataMatrix) # m=행, n=열
numIter=150
weights = ones(n)   #initialize to all ones
for j in range(numIter):
    dataIndex = list(range(m))
    #print(dataMat[j])
    for i in range(m):
        alpha = 4/(1.0+j+i)+0.0001    #apha decreases with iteration, does not 
        randIndex = int(random.uniform(0,len(dataIndex)))#go to 0 because of the constant
        #print(randIndex)
        h = logRegres.sigmoid(sum(dataMatrix[randIndex]*weights))
        error = labelMat[randIndex] - h
        #print(shape(alpha*error*dataMatrix[randIndex]))
        weights = weights+alpha*error*dataMatrix[randIndex]
        del(dataIndex[randIndex])
```

# testSet으로 생존/사망 예측하기
```python
frTest = open('test_adj.csv','r', encoding='utf-8')
errorCount = 0 
numTestVec = 0.0
testResult = []
for line in frTest.readlines():
    numTestVec += 1.0
    currLine = line.strip().split(',')
    lineArr =[]
    for i in range(len(currLine)-1):
        #print(currLine[i])
        lineArr.append(float(currLine[i]))
    if int(logRegres.classifyVector(array(lineArr), weights))!= int(currLine[len(currLine)-1]):
        errorCount += 1
    testResult.append((logRegres.classifyVector(array(lineArr), weights)))
        
errorRate = (float(errorCount)/numTestVec)
print("the error rate of this test is: %f" % errorRate)
f= open('titanic_result.txt','w')
for k in range(0,len(testResult)-1):
    f.write(str(testResult[i])+'\n')
f.close()
```

# 결과

- 예측 값이 맞는지 알 길이 없으니 sub_mission.csv example의 예측값과 비교했는데 0.06정도의 오차율이 나왔다. example은 어떤 방법으로 문제를 풀었는지 궁금하다.혹시 같은 방법...??
