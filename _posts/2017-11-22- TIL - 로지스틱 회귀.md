---
layout:            post
title:             "17/11/22- TIL - Logistic regression"
enutitle:          "17/11/22- TIL - Logistic regression"
category:          Machine learning
author:            myohyun
tags:              Machine learning
---
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

[[TOC]]

# 로지스틱 회귀 Logistic regression

|  | 로지스틱 회귀 Logistic regression |
|--------|--------|
| <b>수집       |모든 방법        |
| <b>준비       |구조적인 데이터 형태가 좋음.(수치형 값은 거리를 계산하는 데 필요)        |
| <b>분석       |모든 방법        |
| <b>훈련       |훈련하는 데 대부분의 시간을 보내며, <br>이 단계에서 데이터를 분류하기 위해 최적의 계수 찾음        |
| <b>검사       |훈련 단계를 마치고 나면 분류는 빠르고 쉽게 진행됨        |
| <b>사용       |이 응용 프로그램은 양간의 입력 데이터가 있어야 함<br>1. 수치형으로 구성된 값을 출력<br>2. 응용 프로그램은 입력 데이터에 간단한 회귀 계산을 적용<br>3. 입력 데이터가 속하는 분류 학목을 결정함<br>또한,응용 프로그램은 계산된 분류 항목에서 몇 가지 동작을 수행함|
| <b>장점       |1. 계산 비용이 적다.<br>2. 구현하기 쉽다<br>3. 결과 해석을 위한 지식 표현이 쉽다        |
| <b>단점       |2. 언더피팅(underfitting)경향이 있어, 정확도가 낮게 나올 수 있음        |
| <b>활용       |3. 수치형 값, 명목형 값        |

## 요약
- 로지스틱 회귀는 시그모이드라고 하는 비선형 함수의 최적의 매개변수를 찾는다.

- 최적의 매개변수를 찾기 위해 최적화알고리즘을 사용한다. 이 때 일반적인 알고리즘은 기울기 상승-> 확률 기울기 상승으로 단순화 가능

- 확률 기울기 상승(온라인 알고리즘)은 비교적 적은 계산으로 기울기 상승만큼 최적화 잘 할 수 있음. 새로 들어온 데이터만을 학습하여 갱신가능

- 기계학습에서 중요한 문제중 하나: 누락된 데이터를 다루는 방법. 실질적으로 데이터를 가지고 무엇을 하고자 하는지에 의존한다. 이것을 해결하는 방법은 여러 가지가 있으며, 각각의 해결 방법은 그 방법들만의 장점과 단점이 있다.



? 두 개 이상의 분류가 가능한가? 가능한 듯 하다. 위의 챕터에서 다루는 듯 함

## weight 구하기

- 데이터 준비

**데이터 벡터**: $$ x $$ (깃허브 올릴 땐 인라인도 \$\$로 )<br>
**weight**: $$ w $$<br>
**데이터 행렬**: $$ M $$<br>
**데이터 분류값 행렬**: $$ t $$<br>


- weight 구하기
이때 가중치를 구하는 방법은 다양하다. 첫 번째로 weight를 최적화 하기 위해 `gradient method`를 사용해본다.


for loop 안. 반복 횟수를 정해준다.-------------------------<br>

$$ z = x^Tw $$ 를 계산한다.

sigmoid 함수 = $$ \sigma(z) = \frac{1}{1+e^{-z}} $$ 를 이용해 $$ h= \sigma(z) $$를 구한다.
각 데이터 분류값 벡터 $$ c $$ 에서 $$ h $$ 를 빼 `error` 벡터를 구한다.<br>
$$ e = c - h $$

weight를 업데이트한다.<br>
$$ w = w + \alpha M^Te $$
<!-- $$ \sigma(z)> 0.5 $$ 이면 1, $$ \sigma(z) \leq 0.5 $$ 이면 0으로 분류 --> 

for loop-------------------------------------------------------------

```c
def gradAscent(dataMatIn, classLabels):
    dataMatrix = mat(dataMatIn)             #convert to NumPy matrix
    labelMat = mat(classLabels).transpose() #convert to NumPy matrix
    m,n = shape(dataMatrix)
    alpha = 0.001
    maxCycles = 500
    weights = ones((n,1))
    for k in range(maxCycles):              #heavy on matrix operations
        h = sigmoid(dataMatrix*weights)     #matrix mult
        error = (labelMat - h)              #vector subtraction
        weights = weights + alpha * dataMatrix.transpose()* error #matrix mult
    return weights
```


계산을 끝낸 $$ w $$ 값으로 의사결정 경계선을 그립니다. 경계선이 두 집합을 잘 분리하는지 확인!<br>

$$
 y= - \frac{w[0]}{w[2]} -\frac{w[1]}{w[2]}x
$$

- ### 결과
<figure>
   <img src="{{ '/media/img/logistic1.png' | absolute_url }}" />
   <figcaption>두 집합을 분류하는 직선을 구함</figcaption>
</figure>


분류는 잘 되었으나, 너무 많은 계산을 하므로 (데이터가 1억, 10억 개 이상 된다면?) 이를 보강하는 방법을 생각해 볼 수 있다. -> 확률적인 기울기 상승법

## 확률 기울기 상승

- 온라인 학습 알고리즘 중 하나. 이런 이름으로 알려져 있는 이유는 모든 데이터를 한꺼번에 처리하지 않고, 새로운 데이터가들어돌 때마다 분류기를 점진적으로 갱신할 수 있기 때문. (데이터를 한꺼번에 처리하는 방법은 일괄 처리(batch processing))
- 한 번에 단 하나의 사례를 사용하여 가중치를 갱신하는 방법

#### 처음 코드를 수정
- 위에서 수행한 방법에서 h와 e가 벡터가 아닌 단일값이 된다. 데이터행렬도 벡터로 바뀜.

```c
def stocGradAscent0(dataMatrix, classLabels):
    m,n = shape(dataMatrix)
    alpha = 0.01
    weights = ones(n)   #initialize to all ones
    for i in range(m):
        h = sigmoid(sum(dataMatrix[i]*weights))
        error = classLabels[i] - h
        weights = weights + alpha * error * dataMatrix[i]
    return weights
```

<figure>
   <img src="{{ '/media/img/logistic2.png' | absolute_url }}" />
   <figcaption>온라인 학습 알고리즘 적용시</figcaption>
</figure>


- w를 업데이트할 때 데이터를 하나 밖에 사용을 안 해서 위의 w보다는 값의 정확도가 떨어진다. 하지만 최적화 알고리즘이 얼마나 잘 분류하는지 확인하기 위한 방법은 코드의 직접적인 비교보다는 이 w가 수렴하는지 확인해야 한다.


#### 수정된 온라인 학습 알고리즘

**정확도를 위한 개선사항:**
확률기울기 상승 알고리즘에서 가중치의 변화는 증폭값인 $$ \alpha $$ 와 <br>
랜덤으로 데이터값을 선택한다. 또한 numIter 값을 매개변수로 설정하여 이중for문을 구성한다.

**$$ \alpha $$**:  알파와 진폭은 비례<br>
**랜덤데이터 선택**: 알고리즘이 이중 for문으로 되어있다. 랜덤으로 선택하지 않는다면 똑같은 데이터 순서로 w를 업데이트 하기 때문에 똑같은 모양이 주기적으로 나타나게 된다.<br>

```c
for j in range(numIter):
        dataIndex = range(m)
        for i in range(m):
            alpha = 4/(1.0+j+i)+0.0001    #apha decreases with iteration, does not 
            randIndex = int(random.uniform(0,len(dataIndex)))#go to 0 because of the constant
            h = sigmoid(sum(dataMatrix[randIndex]*weights))
            error = classLabels[randIndex] - h
            weights = weights + alpha * error * dataMatrix[randIndex]
            del(dataIndex[randIndex])
```


<figure>
   <img src="{{ '/media/img/logistic3-1.png' | absolute_url }}" />
   <img src="{{ '/media/img/logistic3-2.png' | absolute_url }}" />
   <img src="{{ '/media/img/logistic3-3.png' | absolute_url }}" />
   <figcaption>위:알파가 점점 줄어듬<br>중:알파 0.001<br>아래: 알파 3</figcaption>
</figure>

- ### 결과

맨 처음 행렬계산보다 계산량은 줄었지만 정확도는 올라갔다.

<figure>
   <img src="{{ '/media/img/logistic4.png' | absolute_url }}" />
   <figcaption>개선된 온라인 학습 알고리즘</figcaption>
</figure>
