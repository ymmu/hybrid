---
layout:            post
title:             "17/12/04 - TIL - 데이터 클리닝,조사,매칭, 서식화"
menutitle:         "17/12/04 - TIL - 데이터 클리닝,조사,매칭, 서식화"
category:          data analysis
author:            myohyun
tags:              python
---

(파이썬을 활용한 데이터길들이기 7장 참고)

**데이터 랭글링**(Data Wrangling) 혹은 데이터 먼징(Data Munging)은 원자료(raw data)를 또다른 형태로 수작업으로 전환하거나 [매핑](https://ko.wikipedia.org/wiki/%EC%82%AC%EC%83%81_(%EC%BB%B4%ED%93%A8%ED%8C%85))하는 과정이다. 이를 통해서 반자동화 도구의 도움으로 데이터를 좀더 편리하게 소비한다. 데이터 랭글링에는 먼징(munging), [데이터 시각화](https://ko.wikipedia.org/wiki/%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%8B%9C%EA%B0%81%ED%99%94), 데이터 집합, 통계모형 학습 뿐만 아니라 많은 다른 잠재적 용도도 포함된다.

일반적으로 데이터 먼징은 일반적인 단계를 따르는데 데이터 원천(Data Source)으로부터 원래 최초 형태로 자료를 추출하는 것으로 시작한다. [알고리듬](https://ko.wikipedia.org/wiki/%EC%95%8C%EA%B3%A0%EB%A6%AC%EB%93%AC)(예로, 정렬)을 사용해서 원자료를 "먼징(munging"하거나 사전 정의된 자료구조로 데이터를 [파싱](https://ko.wikipedia.org/wiki/%EA%B5%AC%EB%AC%B8_%EB%B6%84%EC%84%9D)(parsing)한다. 그리고 나서 마지막으로 저장이나 미래 사용을 위해서 작업완료한 콘텐츠를 데이터 싱크(data sink)에 놓아둔다. 인터넷의 급격한 확산으로 이러한 기술이 가용한 데이터 양이 증가하고 있는 기관에서는 점점 중요해지고 있다.

데이터 랭글러(Data Wrangler)는 랭글링을 수행하는 사람이다.

# 데이터 클리닝하기: 조사, 매칭, 서식화

**GOAL** 

1. 파이썬을 이용한 데이터 클리닝 및 서식화(포맷팅)방법을 살펴본다.
2. 데이터셋 내의 중복 기록과 오류를 찾는 방법을 알아본다.
* (8장)클리닝 작업을 자동화 하고 클리닝을 마친 데이터를 저장하는 방법을 볼 것임

## 왜 데이터 클리닝을 하는가? 

- 데이터가 클리닝 되어있다 하더라도 서식 비일관성이나 가독성의 문제가 있을 수 있다
- 데이터 클리닝 작업을 하면 데이터를 쉽게 저장하고 검색하고 재사용할 수 있다.

*데이터클리닝 절차를 공개하면 데이터셋 자체와 연구에서 데이터셋의 용도를 제대로 변호할 수 있다. 또한 새로운 데이터를 획득했을 때 동일한 클리닝 절차를 반복하여 적용할 수 있다.

## 데이터 클리닝 기초

**데이터를 수정하고 표준화하여 새로운 데이터 형식을 만드는 작업 :** 

우왕 이거 엄청 귀찮아..예의 유니세프 데이터에서는 column의 이름이 알파벳조합으로 이루어져 알 수 없었다. 따라서 좀 더 이해하기 쉽게 적혀진 column으로 대체하는 작업을 하는데, column데이터 해석 데이터와 유니세프  column 수가 맞지 않아서 어떤 게 원데이터에 포함 안 되어 있는건지, 그렇게 안 들어가는거 빼고 대입시켰는데 원데이터에 여전히 안 바뀌어진 column이 있어서 그거는 왜 그런지 확인하고..등등등 

불필요한 공백 없애기, 길이 기준으로 데이터 나열하기, 수식 실행하기 등등 사람이 보기 좋게 만드는 작업

**이상치와 불량 데이터 찾기**

- 데이터 클리닝은 데이터르 조작하거나 바꾸는 작업을 나타내는 것이 아니다, 그렇기 때문에 이상치나 불량 기록을 삭제하려고 한다면 우선 이에 대해 충분히 고민할 것. 데이터를 정규화하기위해 이상치를 제거했다면 최종 결론 보고 시 그것에 대해 분명하게 밝혀야 한다.

- 데이터의 타당선은 데이터의 출처에서부터 온다. 6장에서 살펴 본 것처럼 데이터 출처에 대한 충분한 조사를 통해 믿을 만한 데이터인지 확실히 판단해야 함. 

- 예를 들어 유니세프의 설문조사는 

  - 표준질문형식을 따름

  - 규칙적인 시간 간격을 두고 인구조사를 시행

  - 적절한 인터뷰 수행을 도모하기 위해 근로자 교육에 관한 표준 지침도 존재

  - 만약 촌락 인구를 무시하고 대도시에 거주하는 가족만을 대상으로 인터뷰를 진행했다면 선택 편향이나 표집 편향과 같은 오류가 발생했을 수도 있음. **편향오류를 확인해봐야**

    > 데이터에 존재할 수 있는 표집현향에 대해서 항상 인식하고, 모집단 전체를 대표하지 못할 가능성이 있는 데이터세트를 기반으로 무분벼란 주장을 하지 않도록 주의해야 함

1. 결측데이터 조사 (for  문 돌려봄)

2. 유형이상치(type outlier): 예를 들어 연도 엔트리에 "missing" 이나 'NA'와 같은 문자열이 들어 있다면 해당 엔트리가 유형 이상치임을 의심할 수 있다. 만약 몇몇 엔트리의 데이터 유형이 나머지 엔트리의 유형과 일치하지 않는다면 해당 엔트리를 이상치 혹은 불량 데이터로 의심해 볼 수 있다. 그러나 이러한 경우가 많다면 해당 데이터 사용을 재고해 보거나 '불량데이터' 패턴이 나타나는 이유에 대해 생각해보아야 한다.

   만약 데이터의 비일관성이 어떻게 야기되었는지 쉽게 파악할 수 있다면 해당 데이터를 사용해도 무방. 그러나 연구 결과데 지대한 영향을 미칠 수 있는 질문에 대한 응답 가운데 비일관성이 존재하고 그 비일관성의 원인을 명확하게 설명할 수 없다면, 설명이 가능할 때까지 해당 데이터셋을 계속해서 조사해보거나 혹은 데이터 불량의원인을 설명할 수 있는 다른 데이터셋을 찾아보아야 한다.( 이 부분은 9장에서 더 자세히 다룬다)

   **데이터유형 살펴보기** : 예를 들면 출생년도와 같이 숫자로 표기될 것이라고 예측되는 응답의 경우 데이터 유형이 제대로 설정되어 있는지 확인해야 함. ->숫자형에 숫자유형이 아닌 것은 NA 이거나 잘못된 값이면 null등으로 바꿔 데이터 표준화시켜야 함. **통계적 추측을 할 때는 표준화가 더욱 필요함**

   > 데이터셋을 계속해서 활용하다보면 이례적인 데이터 유형이나 NA 응답을 볼 수 있다. 데이터셋에 존재하는 이러한 비일관선을 처리하는 방법은 연구 주제와 데이터셋에 대한 지식, 연국 질문에 따라 달라질 수 있다. 데이터셋을 결합한다면 이상치와 불량한 데이터 패턴을 제거하는 것도 하나의 처리 방법이 될 수 있는데, 잘 드러나지 않지만 데이터셋에 존재하는 추세를 간과하지 않도록 주의

3. 중복기록 찾기 : 중복을 없애는 set()함수/ numpy 의 unique  등을 이용함

4. 퍼지매칭: 두 개 이상의 데이터셋을 사용하고 있거나 깔끔하지 않고 통일성이 없는 데이터를 사용하고 있다면 퍼지 매칭을 활용하여 중복 기록을 찾아 결함할 수 있음.퍼지 매칭을 사용하면 두 항목(보통 문자열) 이 동일한지 판단할 수 있다. 자연어 처리나 머신러닝처럼 심도있는 방법은 아니지만 퍼지 매칭을 이용하면 "나의 개와 나(My dog & I )" 와 "나와 나의 개(me and my dog)"의 두항목이 비슷한 의미를 가졌다고 연결 지을 수 있다. 

5. 정규식 매칭: 

6. 중복기록 처리하기: 데이터의 상태에 따라 중복 기록들을 결합해야 할 때도 있다. 데이터셋에 단순히 중복된 행이 존재한다면 데이터를 보존할 필요는 없다. 이미 완성된 데이터셋의 일부이기 대문에 클리닝된 데이터에서 이러한 행을 제거해버리면 된다. 그러나 만약 여러 개의 데이터셋을 결합했고, 중복 기록들을 보존하고 싶다면 파이썬을 이용하여 이를 보존하는 방법을 알아두어야 한다.
